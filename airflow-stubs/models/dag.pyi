import functools
import jinja2
import pathlib
import pendulum
from _typeshed import Incomplete
from airflow import settings as settings, utils as utils
from airflow.api_internal.internal_api_call import internal_api_call as internal_api_call
from airflow.configuration import secrets_backend_list as secrets_backend_list
from airflow.datasets import BaseDataset as BaseDataset, Dataset as Dataset, DatasetAlias as DatasetAlias, DatasetAll as DatasetAll
from airflow.datasets.manager import dataset_manager as dataset_manager
from airflow.decorators import TaskDecoratorCollection as TaskDecoratorCollection
from airflow.exceptions import AirflowDagInconsistent as AirflowDagInconsistent, AirflowException as AirflowException, DuplicateTaskIdFound as DuplicateTaskIdFound, FailStopDagInvalidTriggerRule as FailStopDagInvalidTriggerRule, ParamValidationError as ParamValidationError, RemovedInAirflow3Warning as RemovedInAirflow3Warning, TaskDeferred as TaskDeferred, TaskNotFound as TaskNotFound, UnknownExecutorException as UnknownExecutorException
from airflow.executors.executor_loader import ExecutorLoader as ExecutorLoader
from airflow.jobs.job import run_job as run_job
from airflow.models.abstractoperator import AbstractOperator as AbstractOperator, TaskStateChangeCallback as TaskStateChangeCallback
from airflow.models.base import Base as Base, StringID as StringID
from airflow.models.baseoperator import BaseOperator as BaseOperator
from airflow.models.dagbag import DagBag as DagBag
from airflow.models.dagcode import DagCode as DagCode
from airflow.models.dagpickle import DagPickle as DagPickle
from airflow.models.dagrun import DagRun as DagRun, RUN_ID_REGEX as RUN_ID_REGEX
from airflow.models.dataset import DatasetAliasModel as DatasetAliasModel, DatasetDagRunQueue as DatasetDagRunQueue, DatasetModel as DatasetModel
from airflow.models.operator import Operator as Operator
from airflow.models.param import DagParam as DagParam, ParamsDict as ParamsDict
from airflow.models.serialized_dag import SerializedDagModel as SerializedDagModel
from airflow.models.slamiss import SlaMiss as SlaMiss
from airflow.models.taskinstance import Context as Context, TaskInstance as TaskInstance, TaskInstanceKey as TaskInstanceKey, clear_task_instances as clear_task_instances
from airflow.models.tasklog import LogTemplate as LogTemplate
from airflow.secrets.local_filesystem import LocalFilesystemBackend as LocalFilesystemBackend
from airflow.security import permissions as permissions
from airflow.serialization.pydantic.dag import DagModelPydantic as DagModelPydantic
from airflow.serialization.pydantic.dag_run import DagRunPydantic as DagRunPydantic
from airflow.settings import json as json
from airflow.stats import Stats as Stats
from airflow.timetables.base import DagRunInfo as DagRunInfo, DataInterval as DataInterval, TimeRestriction as TimeRestriction, Timetable as Timetable
from airflow.timetables.interval import CronDataIntervalTimetable as CronDataIntervalTimetable, DeltaDataIntervalTimetable as DeltaDataIntervalTimetable
from airflow.timetables.simple import ContinuousTimetable as ContinuousTimetable, DatasetTriggeredTimetable as DatasetTriggeredTimetable, NullTimetable as NullTimetable, OnceTimetable as OnceTimetable
from airflow.timetables.trigger import CronTriggerTimetable as CronTriggerTimetable
from airflow.typing_compat import Literal as Literal
from airflow.utils import timezone as timezone
from airflow.utils.dag_cycle_tester import check_cycle as check_cycle
from airflow.utils.dates import cron_presets as cron_presets
from airflow.utils.decorators import fixup_decorator_warning_stack as fixup_decorator_warning_stack
from airflow.utils.helpers import at_most_one as at_most_one, exactly_one as exactly_one, validate_instance_args as validate_instance_args, validate_key as validate_key
from airflow.utils.log.logging_mixin import LoggingMixin as LoggingMixin
from airflow.utils.session import NEW_SESSION as NEW_SESSION, provide_session as provide_session
from airflow.utils.sqlalchemy import Interval as Interval, UtcDateTime as UtcDateTime, lock_rows as lock_rows, tuple_in_condition as tuple_in_condition, with_row_locks as with_row_locks
from airflow.utils.state import DagRunState as DagRunState, State as State, TaskInstanceState as TaskInstanceState
from airflow.utils.task_group import TaskGroup as TaskGroup
from airflow.utils.trigger_rule import TriggerRule as TriggerRule
from airflow.utils.types import ArgNotSet as ArgNotSet, DagRunType as DagRunType, EdgeInfoType as EdgeInfoType, NOTSET as NOTSET
from collections import abc as abc
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from pendulum.tz.timezone import FixedTimezone as FixedTimezone, Timezone as Timezone
from sqlalchemy.orm.query import Query as Query
from sqlalchemy.orm.session import Session as Session
from types import ModuleType
from typing import Any, Callable, Collection, Container, Iterable, Iterator, Pattern

log: Incomplete
DEFAULT_VIEW_PRESETS: Incomplete
ORIENTATION_PRESETS: Incomplete
TAG_MAX_LEN: int
DagStateChangeCallback = Callable[[Context], None]
ScheduleInterval = None | str | timedelta | relativedelta
ScheduleIntervalArg = ArgNotSet | ScheduleInterval
ScheduleArg: Incomplete
SLAMissCallback: Incomplete
DEFAULT_SCHEDULE_INTERVAL: Incomplete

class InconsistentDataInterval(AirflowException):
    def __init__(self, instance: Any, start_field_name: str, end_field_name: str) -> None: ...

def create_timetable(interval: ScheduleIntervalArg, timezone: Timezone | FixedTimezone) -> Timetable: ...
def get_last_dagrun(dag_id, session, include_externally_triggered: bool = False): ...
def get_dataset_triggered_next_run_info(dag_ids: list[str], *, session: Session) -> dict[str, dict[str, int | str]]: ...

DAG_ARGS_EXPECTED_TYPES: Incomplete

class DAG(LoggingMixin):
    fileloc: str
    parent_dag: DAG | None
    owner_links: Incomplete
    user_defined_macros: Incomplete
    user_defined_filters: Incomplete
    default_args: Incomplete
    params: Incomplete
    task_dict: dict[str, Operator]
    timezone: Timezone | FixedTimezone
    start_date: Incomplete
    end_date: Incomplete
    schedule_interval: ScheduleInterval
    timetable: Incomplete
    template_searchpath: Incomplete
    template_undefined: Incomplete
    last_loaded: datetime
    safe_dag_id: Incomplete
    max_active_runs: Incomplete
    max_consecutive_failed_dag_runs: Incomplete
    dagrun_timeout: Incomplete
    sla_miss_callback: Incomplete
    orientation: Incomplete
    catchup: bool
    partial: bool
    on_success_callback: Incomplete
    on_failure_callback: Incomplete
    edge_info: dict[str, dict[str, EdgeInfoType]]
    has_on_success_callback: bool
    has_on_failure_callback: bool
    is_paused_upon_creation: Incomplete
    auto_register: Incomplete
    fail_stop: bool
    jinja_environment_kwargs: Incomplete
    render_template_as_native_obj: Incomplete
    doc_md: Incomplete
    tags: Incomplete
    def __init__(self, dag_id: str, description: str | None = None, schedule: ScheduleArg = ..., schedule_interval: ScheduleIntervalArg = ..., timetable: Timetable | None = None, start_date: datetime | None = None, end_date: datetime | None = None, full_filepath: str | None = None, template_searchpath: str | Iterable[str] | None = None, template_undefined: type[jinja2.StrictUndefined] = ..., user_defined_macros: dict | None = None, user_defined_filters: dict | None = None, default_args: dict | None = None, concurrency: int | None = None, max_active_tasks: int = ..., max_active_runs: int = ..., max_consecutive_failed_dag_runs: int = ..., dagrun_timeout: timedelta | None = None, sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None, default_view: str = ..., orientation: str = ..., catchup: bool = ..., on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None, on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None, doc_md: str | None = None, params: abc.MutableMapping | None = None, access_control: dict[str, dict[str, Collection[str]]] | dict[str, Collection[str]] | None = None, is_paused_upon_creation: bool | None = None, jinja_environment_kwargs: dict | None = None, render_template_as_native_obj: bool = False, tags: list[str] | None = None, owner_links: dict[str, str] | None = None, auto_register: bool = True, fail_stop: bool = False, dag_display_name: str | None = None) -> None: ...
    def get_doc_md(self, doc_md: str | None) -> str | None: ...
    def validate(self) -> None: ...
    def validate_executor_field(self) -> None: ...
    def validate_setup_teardown(self) -> None: ...
    def __eq__(self, other): ...
    def __ne__(self, other): ...
    def __lt__(self, other): ...
    def __hash__(self): ...
    def __enter__(self): ...
    def __exit__(self, _type: type[BaseException] | None, _value: BaseException | None, _tb: types.TracebackType | None) -> None: ...
    def date_range(self, start_date: pendulum.DateTime, num: int | None = None, end_date: datetime | None = None) -> list[datetime]: ...
    def is_fixed_time_schedule(self): ...
    def following_schedule(self, dttm): ...
    def previous_schedule(self, dttm): ...
    def get_next_data_interval(self, dag_model: DagModel) -> DataInterval | None: ...
    def get_run_data_interval(self, run: DagRun | DagRunPydantic) -> DataInterval: ...
    def infer_automated_data_interval(self, logical_date: datetime) -> DataInterval: ...
    def next_dagrun_info(self, last_automated_dagrun: None | datetime | DataInterval, *, restricted: bool = True) -> DagRunInfo | None: ...
    def next_dagrun_after_date(self, date_last_automated_dagrun: pendulum.DateTime | None): ...
    def iter_dagrun_infos_between(self, earliest: pendulum.DateTime | None, latest: pendulum.DateTime, *, align: bool = True) -> Iterable[DagRunInfo]: ...
    def get_run_dates(self, start_date, end_date: Incomplete | None = None) -> list: ...
    def normalize_schedule(self, dttm): ...
    def get_last_dagrun(self, session=..., include_externally_triggered: bool = False): ...
    def has_dag_runs(self, session=..., include_externally_triggered: bool = True) -> bool: ...
    @property
    def dag_id(self) -> str: ...
    @dag_id.setter
    def dag_id(self, value: str) -> None: ...
    @property
    def is_subdag(self) -> bool: ...
    @property
    def full_filepath(self) -> str: ...
    @full_filepath.setter
    def full_filepath(self, value) -> None: ...
    @property
    def concurrency(self) -> int: ...
    @concurrency.setter
    def concurrency(self, value: int): ...
    @property
    def max_active_tasks(self) -> int: ...
    @max_active_tasks.setter
    def max_active_tasks(self, value: int): ...
    @property
    def access_control(self): ...
    @access_control.setter
    def access_control(self, value) -> None: ...
    @property
    def dag_display_name(self) -> str: ...
    @property
    def description(self) -> str | None: ...
    @property
    def default_view(self) -> str: ...
    @property
    def pickle_id(self) -> int | None: ...
    @pickle_id.setter
    def pickle_id(self, value: int) -> None: ...
    def param(self, name: str, default: Any = ...) -> DagParam: ...
    @property
    def tasks(self) -> list[Operator]: ...
    @tasks.setter
    def tasks(self, val) -> None: ...
    @property
    def task_ids(self) -> list[str]: ...
    @property
    def teardowns(self) -> list[Operator]: ...
    @property
    def tasks_upstream_of_teardowns(self) -> list[Operator]: ...
    @property
    def task_group(self) -> TaskGroup: ...
    @property
    def filepath(self) -> str: ...
    @property
    def relative_fileloc(self) -> pathlib.Path: ...
    @property
    def folder(self) -> str: ...
    @property
    def owner(self) -> str: ...
    @property
    def allow_future_exec_dates(self) -> bool: ...
    def get_concurrency_reached(self, session=...) -> bool: ...
    @property
    def concurrency_reached(self): ...
    def get_is_active(self, session=...) -> None: ...
    def get_is_paused(self, session=...) -> None: ...
    @property
    def is_paused(self): ...
    @property
    def normalized_schedule_interval(self) -> ScheduleInterval: ...
    @staticmethod
    def fetch_callback(dag: DAG, dag_run_id: str, success: bool = True, reason: str | None = None, *, session: Session = ...) -> tuple[list[TaskStateChangeCallback], Context] | None: ...
    def handle_callback(self, dagrun: DagRun, success: bool = True, reason: Incomplete | None = None, session=...): ...
    @classmethod
    def execute_callback(cls, callbacks: list[Callable] | None, context: Context | None, dag_id: str): ...
    def get_active_runs(self): ...
    def get_num_active_runs(self, external_trigger: Incomplete | None = None, only_running: bool = True, session=...): ...
    @staticmethod
    def fetch_dagrun(dag_id: str, execution_date: datetime | None = None, run_id: str | None = None, session: Session = ...) -> DagRun | DagRunPydantic: ...
    def get_dagrun(self, execution_date: datetime | None = None, run_id: str | None = None, session: Session = ...) -> DagRun | DagRunPydantic: ...
    def get_dagruns_between(self, start_date, end_date, session=...): ...
    def get_latest_execution_date(self, session: Session = ...) -> pendulum.DateTime | None: ...
    @property
    def latest_execution_date(self): ...
    @property
    def subdags(self): ...
    def resolve_template_files(self) -> None: ...
    def get_template_env(self, *, force_sandboxed: bool = False) -> jinja2.Environment: ...
    def set_dependency(self, upstream_task_id, downstream_task_id) -> None: ...
    def get_task_instances_before(self, base_date: datetime, num: int, *, session: Session = ...) -> list[TaskInstance]: ...
    def get_task_instances(self, start_date: datetime | None = None, end_date: datetime | None = None, state: list[TaskInstanceState] | None = None, session: Session = ...) -> list[TaskInstance]: ...
    def set_task_instance_state(self, *, task_id: str, map_indexes: Collection[int] | None = None, execution_date: datetime | None = None, run_id: str | None = None, state: TaskInstanceState, upstream: bool = False, downstream: bool = False, future: bool = False, past: bool = False, commit: bool = True, session=...) -> list[TaskInstance]: ...
    def set_task_group_state(self, *, group_id: str, execution_date: datetime | None = None, run_id: str | None = None, state: TaskInstanceState, upstream: bool = False, downstream: bool = False, future: bool = False, past: bool = False, commit: bool = True, session: Session = ...) -> list[TaskInstance]: ...
    @property
    def roots(self) -> list[Operator]: ...
    @property
    def leaves(self) -> list[Operator]: ...
    def topological_sort(self, include_subdag_tasks: bool = False): ...
    def set_dag_runs_state(self, state: DagRunState = ..., session: Session = ..., start_date: datetime | None = None, end_date: datetime | None = None, dag_ids: list[str] | None = None) -> None: ...
    def clear(self, task_ids: Collection[str | tuple[str, int]] | None = None, start_date: datetime | None = None, end_date: datetime | None = None, only_failed: bool = False, only_running: bool = False, confirm_prompt: bool = False, include_subdags: bool = True, include_parentdag: bool = True, dag_run_state: DagRunState = ..., dry_run: bool = False, session: Session = ..., get_tis: bool = False, recursion_depth: int = 0, max_recursion_depth: int | None = None, dag_bag: DagBag | None = None, exclude_task_ids: frozenset[str] | frozenset[tuple[str, int]] | None = ...) -> int | Iterable[TaskInstance]: ...
    @classmethod
    def clear_dags(cls, dags, start_date: Incomplete | None = None, end_date: Incomplete | None = None, only_failed: bool = False, only_running: bool = False, confirm_prompt: bool = False, include_subdags: bool = True, include_parentdag: bool = False, dag_run_state=..., dry_run: bool = False): ...
    def __deepcopy__(self, memo): ...
    def sub_dag(self, *args, **kwargs): ...
    def partial_subset(self, task_ids_or_regex: str | Pattern | Iterable[str], include_downstream: bool = False, include_upstream: bool = True, include_direct_upstream: bool = False): ...
    def has_task(self, task_id: str): ...
    def has_task_group(self, task_group_id: str) -> bool: ...
    @functools.cached_property
    def task_group_dict(self): ...
    def get_task(self, task_id: str, include_subdags: bool = False) -> Operator: ...
    def pickle_info(self): ...
    last_pickled: Incomplete
    def pickle(self, session=...) -> DagPickle: ...
    def tree_view(self) -> None: ...
    def get_tree_view(self) -> str: ...
    @property
    def task(self) -> TaskDecoratorCollection: ...
    task_count: Incomplete
    def add_task(self, task: Operator) -> None: ...
    def add_tasks(self, tasks: Iterable[Operator]) -> None: ...
    def run(self, start_date: Incomplete | None = None, end_date: Incomplete | None = None, mark_success: bool = False, local: bool = False, donot_pickle=..., ignore_task_deps: bool = False, ignore_first_depends_on_past: bool = True, pool: Incomplete | None = None, delay_on_limit_secs: float = 1.0, verbose: bool = False, conf: Incomplete | None = None, rerun_failed_tasks: bool = False, run_backwards: bool = False, run_at_least_once: bool = False, continue_on_failures: bool = False, disable_retry: bool = False) -> None: ...
    def cli(self) -> None: ...
    def test(self, execution_date: datetime | None = None, run_conf: dict[str, Any] | None = None, conn_file_path: str | None = None, variable_file_path: str | None = None, use_executor: bool = False, mark_success_pattern: Pattern | str | None = None, session: Session = ...) -> DagRun: ...
    def create_dagrun(self, state: DagRunState, execution_date: datetime | None = None, run_id: str | None = None, start_date: datetime | None = None, external_trigger: bool | None = False, conf: dict | None = None, run_type: DagRunType | None = None, session: Session = ..., dag_hash: str | None = None, creating_job_id: int | None = None, data_interval: tuple[datetime, datetime] | None = None): ...
    @classmethod
    def bulk_sync_to_db(cls, dags: Collection[DAG], session=...): ...
    @classmethod
    def bulk_write_to_db(cls, dags: Collection[DAG], processor_subdir: str | None = None, session=...): ...
    def sync_to_db(self, processor_subdir: str | None = None, session=...): ...
    def get_default_view(self): ...
    @staticmethod
    def deactivate_unknown_dags(active_dag_ids, session=...) -> None: ...
    @staticmethod
    def deactivate_stale_dags(expiration_date, session=...) -> None: ...
    @staticmethod
    def get_num_task_instances(dag_id, run_id: Incomplete | None = None, task_ids: Incomplete | None = None, states: Incomplete | None = None, session=...) -> int: ...
    @classmethod
    def get_serialized_fields(cls): ...
    def get_edge_info(self, upstream_task_id: str, downstream_task_id: str) -> EdgeInfoType: ...
    def set_edge_info(self, upstream_task_id: str, downstream_task_id: str, info: EdgeInfoType): ...
    def validate_schedule_and_params(self) -> None: ...
    def iter_invalid_owner_links(self) -> Iterator[tuple[str, str]]: ...

class DagTag(Base):
    __tablename__: str
    name: Incomplete
    dag_id: Incomplete
    __table_args__: Incomplete

class DagOwnerAttributes(Base):
    __tablename__: str
    dag_id: Incomplete
    owner: Incomplete
    link: Incomplete
    @classmethod
    def get_all(cls, session) -> dict[str, dict[str, str]]: ...

class DagModel(Base):
    __tablename__: str
    dag_id: Incomplete
    root_dag_id: Incomplete
    is_paused_at_creation: Incomplete
    is_paused: Incomplete
    is_subdag: Incomplete
    is_active: Incomplete
    last_parsed_time: Incomplete
    last_pickled: Incomplete
    last_expired: Incomplete
    scheduler_lock: Incomplete
    pickle_id: Incomplete
    fileloc: Incomplete
    processor_subdir: Incomplete
    owners: Incomplete
    description: Incomplete
    default_view: Incomplete
    schedule_interval: Incomplete
    timetable_description: Incomplete
    dataset_expression: Incomplete
    tags: Incomplete
    dag_owner_links: Incomplete
    max_active_tasks: Incomplete
    max_active_runs: Incomplete
    max_consecutive_failed_dag_runs: Incomplete
    has_task_concurrency_limits: Incomplete
    has_import_errors: Incomplete
    next_dagrun: Incomplete
    next_dagrun_data_interval_start: Incomplete
    next_dagrun_data_interval_end: Incomplete
    next_dagrun_create_after: Incomplete
    __table_args__: Incomplete
    parent_dag: Incomplete
    schedule_dataset_references: Incomplete
    schedule_dataset_alias_references: Incomplete
    schedule_datasets: Incomplete
    task_outlet_dataset_references: Incomplete
    NUM_DAGS_PER_DAGRUN_QUERY: Incomplete
    def __init__(self, concurrency: Incomplete | None = None, **kwargs) -> None: ...
    @property
    def next_dagrun_data_interval(self) -> DataInterval | None: ...
    @next_dagrun_data_interval.setter
    def next_dagrun_data_interval(self, value: tuple[datetime, datetime] | None) -> None: ...
    @property
    def timezone(self): ...
    @staticmethod
    def get_dagmodel(dag_id: str, session: Session = ...) -> DagModel | None: ...
    @classmethod
    def get_current(cls, dag_id: str, session=...) -> DagModel | DagModelPydantic: ...
    def get_last_dagrun(self, session=..., include_externally_triggered: bool = False): ...
    def get_is_paused(self, *, session: Session | None = None) -> bool: ...
    def get_is_active(self, *, session: Session | None = None) -> bool: ...
    @staticmethod
    def get_paused_dag_ids(dag_ids: list[str], session: Session = ...) -> set[str]: ...
    def get_default_view(self) -> str: ...
    @property
    def safe_dag_id(self): ...
    @property
    def relative_fileloc(self) -> pathlib.Path | None: ...
    def set_is_paused(self, is_paused: bool, including_subdags: bool = True, session=...) -> None: ...
    def dag_display_name(self) -> str: ...
    @classmethod
    def deactivate_deleted_dags(cls, alive_dag_filelocs: Container[str], processor_subdir: str, session: Session = ...) -> None: ...
    @classmethod
    def dags_needing_dagruns(cls, session: Session) -> tuple[Query, dict[str, tuple[datetime, datetime]]]: ...
    def calculate_dagrun_date_fields(self, dag: DAG, last_automated_dag_run: None | datetime | DataInterval) -> None: ...
    def get_dataset_triggered_next_run_info(self, *, session=...) -> dict[str, int | str] | None: ...

def dag(dag_id: str = '', description: str | None = None, schedule: ScheduleArg = ..., schedule_interval: ScheduleIntervalArg = ..., timetable: Timetable | None = None, start_date: datetime | None = None, end_date: datetime | None = None, full_filepath: str | None = None, template_searchpath: str | Iterable[str] | None = None, template_undefined: type[jinja2.StrictUndefined] = ..., user_defined_macros: dict | None = None, user_defined_filters: dict | None = None, default_args: dict | None = None, concurrency: int | None = None, max_active_tasks: int = ..., max_active_runs: int = ..., max_consecutive_failed_dag_runs: int = ..., dagrun_timeout: timedelta | None = None, sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None, default_view: str = ..., orientation: str = ..., catchup: bool = ..., on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None, on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None, doc_md: str | None = None, params: abc.MutableMapping | None = None, access_control: dict[str, dict[str, Collection[str]]] | dict[str, Collection[str]] | None = None, is_paused_upon_creation: bool | None = None, jinja_environment_kwargs: dict | None = None, render_template_as_native_obj: bool = False, tags: list[str] | None = None, owner_links: dict[str, str] | None = None, auto_register: bool = True, fail_stop: bool = False, dag_display_name: str | None = None) -> Callable[[Callable], Callable[..., DAG]]: ...

STATICA_HACK: bool

class DagContext:
    autoregistered_dags: set[tuple[DAG, ModuleType]]
    current_autoregister_module_name: str | None
    @classmethod
    def push_context_managed_dag(cls, dag: DAG): ...
    @classmethod
    def pop_context_managed_dag(cls) -> DAG | None: ...
    @classmethod
    def get_current_dag(cls) -> DAG | None: ...
