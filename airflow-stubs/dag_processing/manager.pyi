import enum
import os
from _typeshed import Incomplete
from airflow.api_internal.internal_api_call import internal_api_call as internal_api_call
from airflow.callbacks.callback_requests import CallbackRequest as CallbackRequest, SlaCallbackRequest as SlaCallbackRequest
from airflow.configuration import conf as conf
from airflow.dag_processing.processor import DagFileProcessorProcess as DagFileProcessorProcess
from airflow.models.dag import DagModel as DagModel
from airflow.models.dagbag import DagPriorityParsingRequest as DagPriorityParsingRequest
from airflow.models.dagwarning import DagWarning as DagWarning
from airflow.models.db_callback_request import DbCallbackRequest as DbCallbackRequest
from airflow.models.errors import ParseImportError as ParseImportError
from airflow.models.serialized_dag import SerializedDagModel as SerializedDagModel
from airflow.secrets.cache import SecretCache as SecretCache
from airflow.stats import Stats as Stats
from airflow.traces.tracer import Trace as Trace, span as span
from airflow.utils import timezone as timezone
from airflow.utils.dates import datetime_to_nano as datetime_to_nano
from airflow.utils.file import list_py_file_paths as list_py_file_paths, might_contain_dag as might_contain_dag
from airflow.utils.log.logging_mixin import LoggingMixin as LoggingMixin
from airflow.utils.mixins import MultiprocessingStartMethodMixin as MultiprocessingStartMethodMixin
from airflow.utils.net import get_hostname as get_hostname
from airflow.utils.process_utils import kill_child_processes_by_pids as kill_child_processes_by_pids, reap_process_group as reap_process_group, set_new_process_group as set_new_process_group
from airflow.utils.retries import retry_db_transaction as retry_db_transaction
from airflow.utils.session import NEW_SESSION as NEW_SESSION, provide_session as provide_session
from airflow.utils.sqlalchemy import prohibit_commit as prohibit_commit, with_row_locks as with_row_locks
from datetime import datetime, timedelta
from multiprocessing.connection import Connection as MultiprocessingConnection
from sqlalchemy.orm import Session as Session
from typing import Any, Callable, NamedTuple

class DagParsingStat(NamedTuple):
    done: bool
    all_files_processed: bool

class DagFileStat(NamedTuple):
    num_dags: int
    import_errors: int
    last_finish_time: datetime | None
    last_duration: timedelta | None
    run_count: int
    last_num_of_db_queries: int

class DagParsingSignal(enum.Enum):
    AGENT_RUN_ONCE = 'agent_run_once'
    TERMINATE_MANAGER = 'terminate_manager'
    END_MANAGER = 'end_manager'

class DagFileProcessorAgent(LoggingMixin, MultiprocessingStartMethodMixin):
    def __init__(self, dag_directory: os.PathLike, max_runs: int, processor_timeout: timedelta, dag_ids: list[str] | None, pickle_dags: bool, async_mode: bool) -> None: ...
    def start(self) -> None: ...
    def run_single_parsing_loop(self) -> None: ...
    def get_callbacks_pipe(self) -> MultiprocessingConnection: ...
    def wait_until_finished(self) -> None: ...
    def heartbeat(self) -> None: ...
    @property
    def done(self) -> bool: ...
    @property
    def all_files_processed(self): ...
    def terminate(self) -> None: ...
    def end(self) -> None: ...

class DagFileProcessorManager(LoggingMixin):
    DEFAULT_FILE_STAT: Incomplete
    standalone_dag_processor: Incomplete
    print_stats_interval: Incomplete
    last_dag_dir_refresh_time: Incomplete
    last_stat_print_time: int
    last_deactivate_stale_dags_time: Incomplete
    parsing_cleanup_interval: Incomplete
    stale_dag_threshold: Incomplete
    dag_dir_list_interval: Incomplete
    waitables: dict[Any, MultiprocessingConnection | DagFileProcessorProcess]
    heartbeat: Callable[[], None]
    def __init__(self, dag_directory: os.PathLike[str], max_runs: int, processor_timeout: timedelta, dag_ids: list[str] | None, pickle_dags: bool, signal_conn: MultiprocessingConnection | None = None, async_mode: bool = True) -> None: ...
    def register_exit_signals(self) -> None: ...
    def start(self): ...
    @classmethod
    def deactivate_stale_dags(cls, last_parsed: dict[str, datetime | None], dag_directory: str, stale_dag_threshold: int, session: Session = ...): ...
    @staticmethod
    def clear_nonexistent_import_errors(file_paths: list[str] | None, processor_subdir: str | None, session=...): ...
    def get_pid(self, file_path) -> int | None: ...
    def get_all_pids(self) -> list[int]: ...
    def get_last_runtime(self, file_path) -> float | None: ...
    def get_last_dag_count(self, file_path) -> int | None: ...
    def get_last_error_count(self, file_path) -> int | None: ...
    def get_last_num_of_db_queries(self, file_path) -> int | None: ...
    def get_last_finish_time(self, file_path) -> datetime | None: ...
    def get_start_time(self, file_path) -> datetime | None: ...
    def get_run_count(self, file_path) -> int: ...
    def get_dag_directory(self) -> str: ...
    def set_file_paths(self, new_file_paths) -> None: ...
    def wait_until_finished(self) -> None: ...
    def collect_results(self) -> None: ...
    def start_new_processes(self) -> None: ...
    def add_new_file_path_to_queue(self) -> None: ...
    def prepare_file_path_queue(self) -> None: ...
    def max_runs_reached(self): ...
    def terminate(self) -> None: ...
    def end(self) -> None: ...
    def emit_metrics(self) -> None: ...
    @property
    def file_paths(self): ...

def reload_configuration_for_dag_processing() -> None: ...
