import airflow as airflow
import airflow.models.errors as errors
import airflow.utils.log.logging_mixin
import airflow.utils.mixins
import airflow.utils.timezone as timezone
import enum
import os
from _typeshed import Incomplete
from airflow.api_internal.internal_api_call import internal_api_call as internal_api_call
from airflow.callbacks.callback_requests import CallbackRequest as CallbackRequest, SlaCallbackRequest as SlaCallbackRequest
from airflow.configuration import conf as conf
from airflow.dag_processing.processor import DagFileProcessorProcess as DagFileProcessorProcess
from airflow.models.dag import DagModel as DagModel
from airflow.models.dagwarning import DagWarning as DagWarning
from airflow.models.db_callback_request import DbCallbackRequest as DbCallbackRequest
from airflow.models.serialized_dag import SerializedDagModel as SerializedDagModel
from airflow.secrets.cache import SecretCache as SecretCache
from airflow.stats import Stats as Stats
from airflow.utils.file import list_py_file_paths as list_py_file_paths, might_contain_dag as might_contain_dag
from airflow.utils.log.logging_mixin import LoggingMixin as LoggingMixin
from airflow.utils.mixins import MultiprocessingStartMethodMixin as MultiprocessingStartMethodMixin
from airflow.utils.net import get_hostname as get_hostname
from airflow.utils.process_utils import kill_child_processes_by_pids as kill_child_processes_by_pids, reap_process_group as reap_process_group, set_new_process_group as set_new_process_group
from airflow.utils.retries import retry_db_transaction as retry_db_transaction
from airflow.utils.session import provide_session as provide_session
from airflow.utils.sqlalchemy import prohibit_commit as prohibit_commit, skip_locked as skip_locked, with_row_locks as with_row_locks
from datetime import datetime, timedelta
from typing import ClassVar

TYPE_CHECKING: bool
NEW_SESSION: None

class DagParsingStat(tuple):
    _fields: ClassVar[tuple] = ...
    _field_defaults: ClassVar[dict] = ...
    _fields_defaults: ClassVar[dict] = ...
    _field_types: ClassVar[dict] = ...
    done: Incomplete
    all_files_processed: Incomplete
    def __init__(self, _cls, done: bool, all_files_processed: bool) -> None: ...
    def __getnewargs__(self): ...

class DagFileStat(tuple):
    _fields: ClassVar[tuple] = ...
    _field_defaults: ClassVar[dict] = ...
    _fields_defaults: ClassVar[dict] = ...
    _field_types: ClassVar[dict] = ...
    num_dags: Incomplete
    import_errors: Incomplete
    last_finish_time: Incomplete
    last_duration: Incomplete
    run_count: Incomplete
    def __init__(self, _cls, num_dags: int, import_errors: int, last_finish_time: datetime | None, last_duration: timedelta | None, run_count: int) -> None: ...
    def __getnewargs__(self): ...

class DagParsingSignal(enum.Enum):
    _member_names_: ClassVar[list] = ...
    _member_map_: ClassVar[dict] = ...
    _member_type_: ClassVar[type[object]] = ...
    _value2member_map_: ClassVar[dict] = ...
    AGENT_RUN_ONCE: ClassVar[DagParsingSignal] = ...
    TERMINATE_MANAGER: ClassVar[DagParsingSignal] = ...
    END_MANAGER: ClassVar[DagParsingSignal] = ...
    @classmethod
    def __init__(cls, value) -> None: ...

class DagFileProcessorAgent(airflow.utils.log.logging_mixin.LoggingMixin, airflow.utils.mixins.MultiprocessingStartMethodMixin):
    def __init__(self, dag_directory: os.PathLike, max_runs: int, processor_timeout: timedelta, dag_ids: list[str] | None, pickle_dags: bool, async_mode: bool) -> None: ...
    def start(self) -> None: ...
    def run_single_parsing_loop(self) -> None: ...
    def get_callbacks_pipe(self) -> MultiprocessingConnection: ...
    def wait_until_finished(self) -> None: ...
    def heartbeat(self) -> None: ...
    def terminate(self): ...
    def end(self): ...
    @property
    def done(self): ...
    @property
    def all_files_processed(self): ...

class DagFileProcessorManager(airflow.utils.log.logging_mixin.LoggingMixin):
    DEFAULT_FILE_STAT: ClassVar[DagFileStat] = ...
    def __init__(self, dag_directory: os.PathLike[str], max_runs: int, processor_timeout: timedelta, dag_ids: list[str] | None, pickle_dags: bool, signal_conn: MultiprocessingConnection | None = ..., async_mode: bool = ...) -> None: ...
    def register_exit_signals(self): ...
    def start(self): ...
    @classmethod
    def deactivate_stale_dags(cls, *args, **kwargs): ...
    @staticmethod
    def clear_nonexistent_import_errors(*args, **kwargs): ...
    def get_pid(self, file_path) -> int | None: ...
    def get_all_pids(self) -> list[int]: ...
    def get_last_runtime(self, file_path) -> float | None: ...
    def get_last_dag_count(self, file_path) -> int | None: ...
    def get_last_error_count(self, file_path) -> int | None: ...
    def get_last_finish_time(self, file_path) -> datetime | None: ...
    def get_start_time(self, file_path) -> datetime | None: ...
    def get_run_count(self, file_path) -> int: ...
    def get_dag_directory(self) -> str: ...
    def set_file_paths(self, new_file_paths): ...
    def wait_until_finished(self): ...
    def collect_results(self) -> None: ...
    def start_new_processes(self): ...
    def add_new_file_path_to_queue(self): ...
    def prepare_file_path_queue(self): ...
    def max_runs_reached(self): ...
    def terminate(self): ...
    def end(self): ...
    def emit_metrics(self): ...
    @property
    def file_paths(self): ...
def reload_configuration_for_dag_processing(): ...
